"""
ì‹í’ˆì›ìž¬ë£Œ.csv  â†â†’  ê°€ê³µì‹í’ˆ2500ê±´.csv  ì˜ì–‘ì„±ë¶„ ë§¤ì¹­ ìŠ¤í¬ë¦½íŠ¸
================================================================
ì‹¤í–‰ ë°©ë²•:
  pip install pandas scikit-learn anthropic tqdm

  # LLM ì—†ì´ TF-IDFë§Œ ì‚¬ìš© (ë¹ ë¥´ì§€ë§Œ ì •í™•ë„ ë‚®ìŒ)
  python food_matcher.py

  # Claude API ì‚¬ìš© (ê¶Œìž¥ â€“ ì •í™•ë„ ë†’ìŒ)
  ANTHROPIC_API_KEY=sk-ant-... python food_matcher.py

ì¶œë ¥:
  ì‹í’ˆì›ìž¬ë£Œ_ì˜ì–‘ì„±ë¶„ì¶”ê°€.csv  : ì˜ì–‘ì„±ë¶„ ì»¬ëŸ¼ì´ ì¶”ê°€ëœ ìµœì¢… ê²°ê³¼
  matching_log.csv            : ë§¤ì¹­ ìƒì„¸ ë¡œê·¸ (ê²€ì¦ìš©)
"""

import os, re, json, time
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from tqdm import tqdm

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â–¶ ì„¤ì • (í•„ìš”ì— ë”°ë¼ ìˆ˜ì •)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RAW_CSV        = "ì‹í’ˆì›ìž¬ë£Œ.csv"
DB_CSV         = "ê°€ê³µì‹í’ˆ2500ê±´.csv"
OUTPUT_CSV     = "ì‹í’ˆì›ìž¬ë£Œ_ì˜ì–‘ì„±ë¶„ì¶”ê°€.csv"
LOG_CSV        = "matching_log.csv"

TOP_K               = 5     # TF-IDF í›„ë³´ ìˆ˜
TFIDF_MIN_SCORE     = 0.15  # ì´ ì ìˆ˜ ë¯¸ë§Œì´ë©´ ë§¤ì¹­ ë¶ˆê°€ë¡œ ì²˜ë¦¬ (LLM í˜¸ì¶œ ìƒëžµ)
LLM_BATCH_SIZE      = 20    # 1íšŒ LLM í˜¸ì¶œë‹¹ ì œí’ˆ ìˆ˜
LLM_MODEL           = "claude-sonnet-4-20250514"

# ê°€ì ¸ì˜¬ ì˜ì–‘ì„±ë¶„ ì»¬ëŸ¼
NUTRITION_COLS = [
    "ì—ë„ˆì§€(kcal)",
    "ë‹¨ë°±ì§ˆ(g)",
    "ì§€ë°©(g)",
    "íšŒë¶„(g)",
    "íƒ„ìˆ˜í™”ë¬¼(g)",
    "ë‹¹ë¥˜(g)",
    "ë‚˜íŠ¸ë¥¨(mg)",
    "ì½œë ˆìŠ¤í…Œë¡¤(mg)",
    "í¬í™”ì§€ë°©ì‚°(g)",
    "íŠ¸ëžœìŠ¤ì§€ë°©ì‚°(g)",
    "ì˜ì–‘ì„±ë¶„í•¨ëŸ‰ê¸°ì¤€ëŸ‰",
    "1íšŒì„­ì·¨ì°¸ê³ ëŸ‰",
    "ì‹í’ˆì¤‘ëŸ‰",
]

# ê°€ê³µì‹í’ˆDB ì»¬ëŸ¼ëª…ì´ ë‹¤ë¥¼ ê²½ìš° ë§¤í•‘ (í‚¤=ì‹¤ì œ, ê°’=í‘œì¤€)
COL_ALIAS = {
    "ì—´ëŸ‰(kcal)": "ì—ë„ˆì§€(kcal)",
}


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. ë°ì´í„° ë¡œë“œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_data():
    print("\n[1/4] ë°ì´í„° ë¡œë“œ")
    # ì‹í’ˆì›ìž¬ë£Œ: ì²« í–‰ì´ í•œê¸€ í—¤ë”(ì¤‘ë³µ)ì´ë¯€ë¡œ skiprows=1
    raw = pd.read_csv(RAW_CSV, encoding="utf-8-sig", skiprows=1)
    raw.columns = raw.columns.str.strip()
    raw["ì œí’ˆëª…"] = raw["ì œí’ˆëª…"].fillna("").astype(str).str.strip()
    raw = raw[raw["ì œí’ˆëª…"] != ""].reset_index(drop=True)

    db = pd.read_csv(DB_CSV, encoding="utf-8-sig")
    db = db.rename(columns=COL_ALIAS)
    db.columns = db.columns.str.strip()
    db["ì‹í’ˆëª…"] = db["ì‹í’ˆëª…"].fillna("").astype(str).str.strip()

    print(f"  âœ” ì‹í’ˆì›ìž¬ë£Œ: {len(raw):,}ê±´")
    print(f"  âœ” ê°€ê³µì‹í’ˆDB: {len(db):,}ê±´")

    avail  = [c for c in NUTRITION_COLS if c in db.columns]
    missing = [c for c in NUTRITION_COLS if c not in db.columns]
    if missing:
        print(f"  âš  ê°€ê³µì‹í’ˆDBì— ì—†ëŠ” ì»¬ëŸ¼ (ë¬´ì‹œë¨): {missing}")
    return raw, db, avail


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. TF-IDF ì¸ë±ìŠ¤ + í›„ë³´ ì¶”ì¶œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def normalize(text: str) -> str:
    """íŠ¹ìˆ˜ë¬¸ìž ì œê±° + ì†Œë¬¸ìž ì •ê·œí™”"""
    text = re.sub(r"[^\wê°€-íž£a-zA-Z0-9]", " ", str(text))
    return text.lower().strip()


def build_tfidf(db: pd.DataFrame):
    print("\n[2/4] TF-IDF ì¸ë±ìŠ¤ êµ¬ì¶•")
    corpus = db["ì‹í’ˆëª…"].apply(normalize).tolist()
    vec = TfidfVectorizer(
        analyzer="char_wb",   # ë¬¸ìž n-gram (í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸° ì—†ì´ë„ íš¨ê³¼ì )
        ngram_range=(2, 4),
        min_df=1,
        sublinear_tf=True,
    )
    mat = vec.fit_transform(corpus)
    print(f"  âœ” í–‰ë ¬ í¬ê¸°: {mat.shape[0]:,} Ã— {mat.shape[1]:,}")
    return vec, mat


def get_candidates(query: str, vec, mat, db, top_k=TOP_K):
    q_vec = vec.transform([normalize(query)])
    scores = cosine_similarity(q_vec, mat)[0]
    idx = np.argsort(scores)[::-1][:top_k]
    return [
        {"index": int(i), "ì‹í’ˆëª…": db.iloc[i]["ì‹í’ˆëª…"], "score": float(scores[i])}
        for i in idx
    ]


def extract_all_candidates(raw, db, vec, mat):
    print("  í›„ë³´ ì¶”ì¶œ ì¤‘...")
    all_cands = []
    for name in tqdm(raw["ì œí’ˆëª…"], desc="  TF-IDF"):
        all_cands.append(get_candidates(name, vec, mat, db))
    return all_cands


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. Claude API ë°°ì¹˜ ë§¤ì¹­
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ì‹í’ˆ ë°ì´í„° ë§¤ì¹­ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤.

ìž…ë ¥: JSON ë°°ì—´. ê° ì›ì†ŒëŠ” {"id": ë²ˆí˜¸, "query": ê²€ìƒ‰í• ì œí’ˆëª…, "candidates": [{"index":ìˆ«ìž,"ì‹í’ˆëª…":ì´ë¦„}, ...]}
ì¶œë ¥: JSON ë°°ì—´ë§Œ (ì„¤ëª… ì—†ì´). í˜•ì‹: [{"id": ë²ˆí˜¸, "matched_index": ìˆ«ìžë˜ëŠ”null, "reason": "ê°„ë‹¨í•œì´ìœ "}, ...]

ë§¤ì¹­ ê·œì¹™:
- í•µì‹¬ í’ˆëª©ëª…(ë‹­ê°€ìŠ´ì‚´, ì–´ë¬µ, íŒì½˜, ë‘ë¶€ ë“±)ì´ ê°™ì•„ì•¼ í•¨
- ë§›/í–¥ ì°¨ì´(ë§ˆëŠ˜ë§› vs ì˜¤ë¦¬ì§€ë„)ëŠ” í—ˆìš©
- ë¸Œëžœë“œ ì°¨ì´ëŠ” í—ˆìš©
- ì™„ì „ížˆ ë‹¤ë¥¸ ì‹í’ˆì´ê±°ë‚˜ í›„ë³´ê°€ ëª¨ë‘ ë¶€ì í•©í•˜ë©´ null
- ìœ ì‚¬ë„ê°€ ë‚®ê³  ì—°ê´€ì´ ì—†ìœ¼ë©´ null (ì–µì§€ ë§¤ì¹­ ê¸ˆì§€)"""


def llm_match_batch(client, batch: list) -> list:
    payload = json.dumps(batch, ensure_ascii=False)
    for attempt in range(3):
        try:
            resp = client.messages.create(
                model=LLM_MODEL,
                max_tokens=2048,
                system=SYSTEM_PROMPT,
                messages=[{"role": "user", "content": payload}],
            )
            raw_text = resp.content[0].text.strip()
            raw_text = re.sub(r"```[^\n]*\n?", "", raw_text).strip()
            return json.loads(raw_text)
        except Exception as e:
            print(f"\n  âš  LLM ì˜¤ë¥˜ (ìž¬ì‹œë„ {attempt+1}/3): {e}")
            time.sleep(2 ** attempt)
    # ì‹¤íŒ¨ ì‹œ í´ë°±: top-1 í›„ë³´
    return [
        {"id": b["id"],
         "matched_index": b["candidates"][0]["index"] if b["candidates"] else None,
         "reason": "LLM_FAILED_FALLBACK"}
        for b in batch
    ]


def run_llm_matching(raw, all_cands, client):
    print("  LLM ë§¤ì¹­ ì¤‘...")
    n = len(raw)
    matched_db_idx = [None] * n
    reasons        = [""] * n

    # ìœ ì‚¬ë„ê°€ ë„ˆë¬´ ë‚®ìœ¼ë©´ LLM ìƒëžµ (API ë¹„ìš© ì ˆê°)
    needs_llm = [
        i for i, cands in enumerate(all_cands)
        if cands and cands[0]["score"] >= TFIDF_MIN_SCORE
    ]
    skip_count = n - len(needs_llm)
    print(f"  LLM í˜¸ì¶œ ëŒ€ìƒ: {len(needs_llm):,}ê±´  |  ìžë™ null: {skip_count:,}ê±´ (ìœ ì‚¬ë„ {TFIDF_MIN_SCORE} ë¯¸ë§Œ)")

    batch_input = [
        {
            "id": i,
            "query": raw.iloc[i]["ì œí’ˆëª…"],
            "candidates": [{"index": c["index"], "ì‹í’ˆëª…": c["ì‹í’ˆëª…"]} for c in all_cands[i]],
        }
        for i in needs_llm
    ]

    for start in tqdm(range(0, len(batch_input), LLM_BATCH_SIZE), desc="  LLM ë°°ì¹˜"):
        batch = batch_input[start: start + LLM_BATCH_SIZE]
        results = llm_match_batch(client, batch)
        for r in results:
            idx = r["id"]
            matched_db_idx[idx] = r.get("matched_index")
            reasons[idx]        = r.get("reason", "")
        time.sleep(0.3)  # rate limit ë°©ì§€

    return matched_db_idx, reasons


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. ë³‘í•© & ì €ìž¥
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def merge_and_save(raw, db, all_cands, matched_db_idx, reasons, avail_cols):
    print("\n[4/4] ì˜ì–‘ì„±ë¶„ ë³‘í•© & ì €ìž¥")

    raw["_ë§¤ì¹­ì‹í’ˆëª…"]    = [db.iloc[mi]["ì‹í’ˆëª…"] if mi is not None else "" for mi in matched_db_idx]
    raw["_tfidfìµœê³ ì ìˆ˜"] = [round(cands[0]["score"], 4) if cands else 0.0 for cands in all_cands]
    raw["_ë§¤ì¹­ì´ìœ "]      = reasons

    for col in avail_cols:
        raw[col] = pd.NA

    for i, mi in enumerate(matched_db_idx):
        if mi is not None:
            for col in avail_cols:
                raw.at[i, col] = db.iloc[mi][col]

    raw.to_csv(OUTPUT_CSV, index=False, encoding="utf-8-sig")

    log_cols = ["ì œí’ˆëª…", "_ë§¤ì¹­ì‹í’ˆëª…", "_tfidfìµœê³ ì ìˆ˜", "_ë§¤ì¹­ì´ìœ "] + avail_cols[:4]
    raw[[c for c in log_cols if c in raw.columns]].to_csv(LOG_CSV, index=False, encoding="utf-8-sig")

    matched = sum(1 for m in matched_db_idx if m is not None)
    print(f"\n{'='*52}")
    print(f"  ì „ì²´:       {len(raw):>8,}ê±´")
    print(f"  ë§¤ì¹­ ì„±ê³µ:  {matched:>8,}ê±´  ({matched/len(raw)*100:.1f}%)")
    print(f"  ë§¤ì¹­ ì‹¤íŒ¨:  {len(raw)-matched:>8,}ê±´")
    print(f"{'='*52}")
    print(f"  âœ… ê²°ê³¼: {OUTPUT_CSV}")
    print(f"  ðŸ“‹ ë¡œê·¸: {LOG_CSV}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”ì¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    raw, db, avail_cols = load_data()
    vec, mat = build_tfidf(db)

    print("\n[3/4] ë§¤ì¹­ ì‹¤í–‰")
    all_cands = extract_all_candidates(raw, db, vec, mat)

    api_key = os.environ.get("ANTHROPIC_API_KEY", "").strip()
    if api_key:
        import anthropic
        client = anthropic.Anthropic(api_key=api_key)
        print(f"  âœ” Claude API ì‚¬ìš© ({LLM_MODEL})")
        matched_db_idx, reasons = run_llm_matching(raw, all_cands, client)
    else:
        print("  âš  ANTHROPIC_API_KEY ì—†ìŒ â†’ TF-IDF top-1ìœ¼ë¡œ ìžë™ ì„ íƒ")
        matched_db_idx = [
            cands[0]["index"] if cands and cands[0]["score"] >= TFIDF_MIN_SCORE else None
            for cands in all_cands
        ]
        reasons = [
            f"tfidf:{cands[0]['score']:.3f}" if cands and cands[0]["score"] >= TFIDF_MIN_SCORE else "score_too_low"
            for cands in all_cands
        ]

    merge_and_save(raw, db, all_cands, matched_db_idx, reasons, avail_cols)
